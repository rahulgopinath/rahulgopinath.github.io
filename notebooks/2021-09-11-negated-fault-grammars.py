# ---
# published: true
# title: Specializing Grammars for Not Inducing A Fault
# layout: post
# comments: true
# tags: python
# categories: post
# ---

# In my previous post on [inducing faults](/post/2021/09/09/fault-inducing-grammar/)
# I explained the deficiency of abstract failure inducing inputs mined using
# DDSet, and showed how to overcome that by inserting that abstract (evocative)
# pattern into a grammar, producing evocative grammars that guarantee that the
# evocative fragment is present in any input generated.
#
# As before, let us start with importing our required modules.

import sys, imp
import itertools as I

def make_module(modulesource, sourcestr, modname):
    codeobj = compile(modulesource, sourcestr, 'exec')
    newmodule = imp.new_module(modname)
    exec(codeobj, newmodule.__dict__)
    return newmodule

def import_file(name, location):
    if "pyodide" in sys.modules:
        import pyodide
        github_repo = 'https://raw.githubusercontent.com/'
        my_repo =  'rahulgopinath/rahulgopinath.github.io'
        module_loc = github_repo + my_repo + '/master/notebooks/%s' % location
        module_str = pyodide.open_url(module_loc).getvalue()
    else:
        module_loc = './notebooks/%s' % location
        with open(module_loc) as f:
            module_str = f.read()
    return make_module(module_str, module_loc, name)

# We import the following modules
earleyparser = import_file('earleyparser', '2021-02-06-earley-parsing.py')
hdd = import_file('hdd', '2019-12-04-hdd.py')
fuzzer = import_file('fuzzer', '2019-05-28-simplefuzzer-01.py')
ddset = import_file('ddset', '2020-08-03-simple-ddset.py')
gatleast = import_file('gatleast', '2021-09-09-fault-inducing-grammar.py')
gmultiple = import_file('gmultiple', '2021-09-10-multiiple-fault-grammars.py')

# # A grammar with no fault inducing fragments.
#
# A similar procedure can be used to make sure that no failure inducing
# fragments are present in inputs generated by given grammar. The idea is as
#follows.



# ## Unreachable Grammar
#
# We start with the `get_reachable_positions()` output. If we can ensure
# that no nonterminals in the reachable_positions can actually produce a fault
# inducing fragment, then we are done. So, given the `get_reachable_positions`
# we can produce the unreachable grammar.
# 
# For ease of discussion, we name a
# nonterminal E that is guaranteed to not produce fault tree `F` as `<E neg(F)>`.
# That is, a tree that starts from <start neg(F)> is guaranteed not to contain
# the fault tree `F`.
#
# So, the definition of `<E neg(F)` is simple enough given the characterizing
# node of the fault tree, and the corresponding reaching positions in the
# grammar.
# For each expansion rule of `<E>`, we have to make sure that it does not lead
# to `F`. So rules for `<E>` that did not have reachable positions corresponding
# to characterizing node of `F` can be directly added to `<E neg(F)>`. Next,
# for any rule that contained reachable positions, for all such positions, we
# specialize the nonterminal in that position by `neg(F)`. This gives us the
# unreachable grammar.

def negate_suffix(fault):
    assert fault
    return 'neg(%s)' % fault

def unreachable_key(grammar, key, cnodesym, negated_suffix, reachable):
    rules = grammar[key]
    my_rules = []
    for rule in grammar[key]:
        positions = gatleast.get_reachable_positions(rule, cnodesym, reachable)
        if not positions:
            # not embeddable here. We can add this rule.
            my_rules.append(rule)
        else:
            new_rule = [gatleast.refine_base_key(t, negated_suffix) if p in positions else t for p,t in enumerate(rule)]
            my_rules.append(new_rule)
    return (gatleast.refine_base_key(key, negated_suffix), my_rules)

# Using it.

if __name__ == '__main__':
    reaching = gatleast.reachable_dict(hdd.EXPR_GRAMMAR)
    for key in hdd.EXPR_GRAMMAR:
        fk, rules = unreachable_key(hdd.EXPR_GRAMMAR, key, '<factor>', negate_suffix('F1'), reaching)
        print(fk)
        for r in rules:
            print('    ', r)
        print()


# Next, we can define unreachable grammar using it.

def unreachable_grammar(grammar, start, cnodesym, negated_suffix, reachable):
    new_grammar = {}
    s_key = None
    for key in grammar:
        fk, rules = unreachable_key(grammar, key, cnodesym, negated_suffix, reachable)
        assert fk not in new_grammar
        if key == start: s_key = fk
        new_grammar[fk] = rules
    return new_grammar, s_key

# ## Negated pattern grammar.
# 
# For negated pattern grammars, there are two parts. The first part is for
# pattern rules. The idea is to make sure that we can produce any but not the
# specific pattern in the current expansion. Next, we also need to make sure
# that the original fault is not reachable from any of the nonterminals.

def negate_key(k):
    return '<%s %s>' % (gatleast.stem(k), negate_suffix(gatleast.refinement(k)))

def normalize_grammar(g):
    return {gmultiple.normalize(k):list({tuple([gmultiple.normalize(t) if fuzzer.is_nonterminal(t) else t for t in r]) for r in g[k]}) for k in g}

def rule_normalized_difference(rulesA, rulesB):
    rem_rulesA = rulesA
    for ruleB in rulesB:
        rem_rulesA = [rA for rA in rem_rulesA if not gmultiple.normalized_rule_match(rA, ruleB)]
    return rem_rulesA

def unmatch_a_refined_rule_in_pattern_grammar(refined_rule):
    negated_rules = []
    for pos,token in enumerate(refined_rule):
        if not fuzzer.is_nonterminal(token): continue
        if gatleast.is_base_key(token): continue
        r = [negate_key(t) if i==pos else t for i,t in enumerate(refined_rule)]
        negated_rules.append(r)
    return negated_rules

def unmatch_definition_in_pattern_grammar(refined_rules, base_rules):
    # Given the set of rules, we take one rule at a time,
    # and generate the negated rule set from that.
    negated_rules_refined = []
    for ruleR in refined_rules:
        neg_rules = unmatch_a_refined_rule_in_pattern_grammar(ruleR)
        negated_rules_refined.extend(neg_rules)

    # Finally, we need to add the other non-matching rules to the pattern def.
    negated_rules_base = rule_normalized_difference(base_rules, refined_rules)

    return negated_rules_refined + negated_rules_base


def unmatch_pattern_grammar(pattern_grammar, pattern_start, base_grammar):
    negated_grammar = {}
    for l_key in pattern_grammar:
        l_rule = pattern_grammar[l_key][0]
        nl_key = negate_key(l_key)
        # find all rules that do not match, and add to negated_grammar,
        normal_l_key = gmultiple.normalize(l_key)
        base_rules = base_grammar[normal_l_key]
        refined_rules = pattern_grammar[l_key]

        negated_rules = unmatch_definition_in_pattern_grammar(refined_rules, base_rules)
        negated_grammar[nl_key] = negated_rules
    # this needs to be negated with original fault TODO:
    return {**negated_grammar, **pattern_grammar} , negate_key(pattern_start)

# Using

if __name__ == '__main__':
    pattern_g,pattern_s, t = gatleast.pattern_grammar(gatleast.ETREE_DPAREN, 'F1')
    nomatch_g, nomatch_s = unmatch_pattern_grammar(pattern_g, pattern_s, hdd.EXPR_GRAMMAR)
    # next we need to conjunct
    print('start:', nomatch_s)
    for k in nomatch_g:
        print(k)
        for r in nomatch_g[k]:
            print('    ', r)

# Now, for negated pattern grammars, not only do we need to make sure that the
# pattern is not directly matchable, but also that the pattern cannot be
# embedded. For that we simply conjunct it with `neg(F1)`

def and_suffix(k1, suffix):
    if gatleast.is_base_key(k1):
        return '<%s %s>' % (gatleast.stem(k1), suffix)
    return '<%s and(%s,%s)>' % (gatleast.stem(k1), gatleast.refinement(k1), suffix)

def negated_pattern_grammar(pattern_grammar, pattern_start, base_grammar, nfault_suffix):
    reachable_keys = gatleast.reachable_dict(base_grammar)
    nomatch_g, nomatch_s = unmatch_pattern_grammar(pattern_grammar, pattern_start, base_grammar)

    new_grammar = {}

    my_key = gmultiple.normalize(pattern_start)
    # which keys can reach pattern_start?
    keys_that_can_reach_fault = [k for k in reachable_keys if my_key in reachable_keys[k]]
    #for k in keys_that_can_reach_fault: assert my_key in reachable_keys[k]
    new_g = {}
    for k in nomatch_g: 
        new_rules = []
        for rule in nomatch_g[k]:
            new_rule = [and_suffix(t, nfault_suffix) if t in keys_that_can_reach_fault else t for t in rule]
            new_rules.append(new_rule)
        new_g[k] = new_rules
    return new_g, negate_key(pattern_start)

# Using

if __name__ == '__main__':
    print()
    nomatch_g, nomatch_s = negated_pattern_grammar(pattern_g, pattern_s, hdd.EXPR_GRAMMAR, 'neg(F1)')
    # next we need to conjunct
    print('start:', nomatch_s)
    for k in nomatch_g:
        print(k)
        for r in nomatch_g[k]:
            print('    ', r)


def tokens(g):
    ts = []
    for k in g:
        for r in g[k]:
            for t in r:
                if fuzzer.is_nonterminal(t): ts.append(t)
    return ts
# At this point, we can now define our 1negated_grammar()`
# The new grammar is as follows

def no_fault_grammar(grammar, start_symbol, cnode, fname):
    key_f = cnode[0]
    pattern_g, pattern_s, tr = gatleast.pattern_grammar(cnode, fname)
    negated_suffix = negate_suffix(fname)
    nomatch_g, nomatch_s = negated_pattern_grammar(pattern_g, pattern_s, grammar, negated_suffix)

    reachable_keys = gatleast.reachable_dict(grammar)
    reach_g, reach_s = gatleast.reachable_grammar(grammar, start_symbol, key_f, fname, reachable_keys)
    unreach_g, unreach_s = unreachable_grammar(grammar, start_symbol, key_f, negated_suffix, reachable_keys)

    combined_grammar = {**grammar, **nomatch_g, **reach_g, **unreach_g}
    unreaching_sym = gatleast.refine_base_key(key_f, negated_suffix)
    combined_grammar[unreaching_sym] = unreach_g[unreaching_sym] + nomatch_g[nomatch_s] # TODO verify

    return combined_grammar, unreach_s

# Using it.

if __name__ == '__main__':
    cnode = gatleast.ETREE_DPAREN
    g, s = gatleast.grammar_gc(no_fault_grammar(hdd.EXPR_GRAMMAR, hdd.EXPR_START, cnode, 'F1'))
    print()
    print('start:', s)
    for k in g:
        print(k)
        for r in g[k]:
            print('    ', r)


# This grammar is now guaranteed not to produce any instance of the characterizing node.

if __name__ == '__main__':
    gf = fuzzer.LimitFuzzer(g)
    for i in range(10):
        v = gf.iter_fuzz(key=s, max_depth=10)
        assert hdd.expr_double_paren(v) == hdd.PRes.failed
        print(v)

