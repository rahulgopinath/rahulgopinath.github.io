---
published: true
title: Blackbox Inference of Context Free Grammars -- Verification
layout: post
comments: true
tags: grammar mining, context-free grammar
categories: post
---

The problem of inferring the input specification of a program has recently seen some
interest[^bastani2017synthesizing][^wu2019reinam] from researchers.
This is of huge interest in fuzzing as recovery
of the input specification can improve the efficiency, effectiveness, and generality
of a fuzzer by leaps and bounds[^stevenson2014a].

The best way to go about it is to look for a general solution with proofs. However,
a fully general solution to the problem is impossible, and is as hard as reversing
the RSA encryption[^angluin1995when]. However, this has not stopped researchers from
attempting to look for heuristics which are applicable to context-free grammars that
are found in real-world software. The idea being that, the theoretical limitation
could be about pathological grammars. Even if one could recover the grammar of a
reasonable subset of context-free grammars, it is a win[^curley2016grammatical]. Indeed, Clark et al.[^clark2008a]
takes this approach. Another reasonable approach is to look for approximations. The
research in this field was summarized by Higuera[^higuera2010grammatical].

We need three definitions before we proceed.

* Precision (P): The percentage of inputs that were generated by the synthesized
  grammar that were accepted by the blackbox. That is, synthesized grammar acts as a producer.

* Recall (R): The percentage of inputs that were generated by the blackbox that were
  accepted by the synthesized grammar. That is, synthesized grammar acts as a acceptor.

* F1 score: The F1 score is the harmonic mean of precision and recall, and is
  given by $$ 2 \times \frac{P R}{P + R} $$.

Now, how does one verify their approach? One approach that recent research has taken
is to try and recover the context free grammar from one of the blackbox programs, then
use the recovered grammar to generate inputs, and check how many of these inputs are
accepted by the program under grammar mining[^bastani2017synthesizing][^wu2019reinam] (P).
Now, for completeness, one needs to turn around, and use the grammar as an acceptor
for the input produced by the program (R).
The problem here is that going the other way is tricky. That is, the program under
grammar mining is an acceptor. How do you turn it into a producer? The route that
is taken by Bastani et al. is to write a golden grammar by hand. Then, use this grammar to
generate inputs, which are parsed by the mined grammar [^1]
However, this is unsatisfying and error prone. Is there a better way?

Turns out, there is indeed a better way. Unlike in [whitebox grammar recovery](/resources/fse2020/gopinath2020mining.pdf)
for blackbox grammar inference, there is no particular reason to start with a program.
Instead, start with a context-free grammar of your choice, and use a standard
context-free parser such as GLL, GLR, CYK or Earley parser to turn it into an acceptor. Then, use
your blackbox grammar inference algorithm to recover the grammar. Now, you have the
original grammar, and the recovered grammar. You can now use a number of tools[^madhavan2015automating] to
check how close they are to each other. Indeed, the most basic idea is to make
one grammar the producer, and see how many of the produced is accepted by the second.
Then, turn it around, make the second the producer, and see how many of the produced
is accepted by the first. Make sure to use [random sampling](/post/2021/07/27/random-sampling-from-context-free-grammar/)
for producing inputs from any grammar.
We note that doing both is extremely important to have
confidence in the results. What if we only do one side? That is, only verify that
the inputs produced by the first are accepted by the second? In that case, nothing
prevents us from inferring an extremely permissive grammar for the second that never
rejects any input -- say `/.*/`. This grammar would have 100% accuracy in this testing even though
it is a very poor inferred grammar. Such problems can only be detected if we turn
around and use the inferred grammar as producer. Now, imagine that the inferred grammar
doesn't generalize at all, and produces only a small set of inputs. In that case, again
the original grammar will accept all generated inputs, resulting in 100% accuracy even though
the inferred grammar was bad. Hence, both tests are equally important.

The TLDR; is that **if you are doing blackbox grammar inference, please start with a grammar rather than a program. Use a parser to turn the grammar into an acceptor, and infer the grammar of that acceptor. Then verify both grammars against each other** . Extracting the grammar from a program is not a useful proof for the effectiveness of your technique unless you are doing whitebox grammar mining.

<!-- Now, there is a complication here. For some of the programs such as Perl, or
even [URLS as defined by WhatWG](https://url.spec.whatwg.org/#concept-basic-url-parser),
there is no accepted program specification. Rather, the specification is the
program itself. So, what do you do if you want to check the accuracy of your
inferred grammar? In such a case, you have no other option but to rely on a
handwritten golden grammar. However, you then need to verify that your golden
grammar matches the program in question. To do that, use the golden grammar
to [generate random inputs to a fixed depth](/post/2021/07/27/random-sampling-from-context-free-grammar/).
**Important:** Use [random sampling](/post/2021/07/27/random-sampling-from-context-free-grammar/)
to make sure that you are not biased by the way the golden grammar is written.
Next, verify that all these inputs are accepted by the program from which you
are trying to mine the grammar. Once you have ensured that your golden grammar
is accurate, you can then use this as a proxy input generator for your program.
However, when computing the precision of the synthesized grammar, report the
percentage of inputs that were accepted by both the blackbox program from which
you were trying to learn the grammar, as well as the percentage of inputs that
were accepted by the golden grammar. You have to keep in mind that this will not
protect you from an overly strict golden grammar. That is, if the golden grammar
as well as the synthesized grammar is overly strict -- in the extreme, only
accepts empty strings, it is still possible to get 100% here. So, for a general
evaluation, there is no choice but to start with grammars.
-->

A nice result that I should mention here is that even though comparison of context-free
grammars in general is undecidable[^ginsburg1966the], comparison of deterministic context-free
grammars is decidable![^senizergues2001l]. Géraud Sénizerguese was awarded the Gödel Prize in
2002 for this discovery. What this means is that if the grammars are
deterministic (these are the LR(k) grammars), you can even compare them directly.

The main communities working on grammar inference are
* Language and Automata Theory and Applications ([LATA](https://dblp.org/db/conf/lata/index.html))
* International Conference on Grammatical Inference ([ICGI](https://grammarlearning.org/))
* JMLR


[^1]: We note here that the grammar derived by [GLADE](https://github.com/obastani/glade) is not in the usual format, and hence, we could not verify that their parser is correct. Unfortunately, general context-free parsers are notoriously difficult to get right as shown by the history of the Earley parser.

[^stevenson2014a]: Stevenson, A., & Cordy, J. R. (2014). A survey of grammatical inference in software engineering. Science of Computer Programming, 96, 444-459.

[^bastani2017synthesizing]: Bastani, O., Sharma, R., Aiken, A., & Liang, P. (2017). Synthesizing program input grammars. ACM SIGPLAN Notices, 52(6), 95-110.

[^wu2019reinam]: Wu, Z., Johnson, E., Yang, W., Bastani, O., Song, D., Peng, J., & Xie, T. (2019, August). REINAM: reinforcement learning for input-grammar inference. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (pp. 488-498).

[^angluin1995when]: Angluin, D., & Kharitonov, M. (1995). When Won′ t Membership Queries Help?. Journal of Computer and System Sciences, 50(2), 336-355.

[^clark2008a]: Clark, A., Eyraud, R., & Habrard, A. (2008, September). A polynomial algorithm for the inference of context free languages. In International Colloquium on Grammatical Inference (pp. 29-42). Springer, Berlin, Heidelberg.

[^madhavan2015automating]: Madhavan, R., Mayer, M., Gulwani, S., & Kuncak, V. (2015, October). Automating grammar comparison. In Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications (pp. 183-200).

[^fischer2011comparison]: Fischer, B., Lämmel, R., & Zaytsev, V. (2011, July). Comparison of context-free grammars based on parsing generated test data. In International Conference on Software Language Engineering (pp. 324-343). Springer, Berlin, Heidelberg.

[^senizergues2001l]: Sénizergues, G. (2001). L (A)= L (B)? decidability results from complete formal systems. Theoretical Computer Science, 251(1-2), 1-166.

[^ginsburg1966the]: S. Ginsburg,The Mathematical Theory of Context Free Languages.McGraw-Hill Book Company, 1966.

[^higuera2010grammatical]: C. de la Higuera,Grammatical Inference: Learning Automata andGrammars.  Cambridge University Press, 2010.

[^curley2016grammatical]: Curley, S. S., & Harang, R. E. (2016, May). Grammatical Inference and Machine Learning Approaches to Post-Hoc LangSec. In 2016 IEEE Security and Privacy Workshops (SPW) (pp. 171-178). IEEE.
